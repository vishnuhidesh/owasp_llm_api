from django.http import JsonResponse

def allchallenges(request):
    jsondata = [
    {
        "id": 1,
        "title": "LLM01:2025 Prompt Injection",
        "description": "Prompt Injection is a vulnerability where malicious input manipulates the model's behavior to execute unintended actions. This attack can involve directly altering the LLM's internal instructions or embedding commands in external content, leading to outcomes like data leakage, unauthorized access, or output manipulation.",
        "task": "Here, try to gain access to the command line below the chat section by prompting the chatbot."
    },
    {
        "id": 2,
        "title": "LLM02:2025 Sensitive Information Disclosure",
        "description": "Sensitive Information Disclosure refers to the unintentional release of private or confidential data by large language models (LLMs). This vulnerability arises when the LLM generates outputs that expose sensitive information such as user data, proprietary knowledge, or system configurations. It can occur due to improper access controls, training data leaks, or exploitation of poorly designed prompts.",
        "task": "Here, try to obtain the API key of the chatbot."
    },
    {
        "id": 3,
        "title": "LLM03:2025 Supply Chain",
        "description": "Supply Chain Vulnerability addresses risks stemming from the dependencies and external components integrated into large language model (LLM) applications. These risks include malicious or compromised libraries, tools, or datasets that can introduce vulnerabilities into the LLM's operations. An attacker might exploit these weak links to compromise the model, manipulate its outputs, or leak sensitive data.",
        "task": "Here, first use a good plugin and chat with the bot, after that, use a poisoned plugin and chat with the bot, and compare both results."
    },
    {
        "id": 4,
        "title": "LLM04:2025 Data and Model Poisoning",
        "description": "Data and Model Poisoning refers to malicious alterations in the training data or model parameters of large language models (LLMs). Attackers can introduce biased or harmful data into the training process, causing the model to generate inaccurate, harmful, or manipulative outputs.",
        "task": "Here, first train the model with the good dataset and chat with the bot, then train with the poisoned dataset and chat with the bot, and compare the difference."
    },
    {
        "id": 5,
        "title": "LLM05:2025 Improper Output Handling",
        "description": "Improper Output Handling refers to the failure to properly validate, sanitize, or structure the outputs generated by large language models (LLMs). This vulnerability can result in harmful outcomes such as generating malicious code, exposing sensitive data, or producing misleading information. Attackers can exploit improper handling to manipulate outputs for phishing, impersonation, or other malicious purposes.",
        "task": "Here, ask the bot a few questions on what caused the internal errors and see the response."
    },
    {
        "id": 6,
        "title": "LLM06:2025 Excessive Agency",
        "description": "Excessive Agency describes the risk of large language models (LLMs) being given too much autonomy or control over sensitive systems or processes. When LLMs operate without sufficient oversight, they may execute unintended or harmful actions, such as making unauthorized decisions, exposing vulnerabilities, or automating malicious tasks. This vulnerability arises when LLMs are integrated with high-privilege systems or allowed to perform actions without proper human intervention or safeguards.",
        "task": "From the list of queries, ask any one of that to the chatbot, and see the response."
    },
    {
        "id": 7,
        "title": "LLM07:2025 System Prompt Leakage",
        "description": "System Prompt Leakage occurs when the internal system prompts or instructions used to guide an LLM's behavior are exposed to users or attackers. These prompts often contain sensitive details about the LLM's configuration, rules, or even privileged information. If leaked, they can be exploited to bypass intended restrictions, manipulate the model's behavior, or gain unauthorized insights into its operational logic.",
        "task": "Here, the first list contains some queries to leak system level prompts. The chatbot will not allow to execute that. So prompt the chatbot using the tricky prompts in the second list, which can be used to bypass the security guardrails, then try to prompt it again with the prompts from the first list."
    },
    {
        "id": 8,
        "title": "LLM08:2025 Vector and Embedding Weaknesses",
        "description": "Vector and Embedding Weaknesses refers to vulnerabilities in the way large language models (LLMs) use vector representations and embeddings to process and store data. These weaknesses can be exploited to infer sensitive information, inject malicious vectors, or manipulate downstream tasks. Attackers might use adversarial techniques to create embeddings that distort the LLM's understanding or hijack its outputs.",
        "task": "Here, the model has a weakness, it takes a word form the prompt and fetches the response, which may be quite irrelevant. Try prompting the bot."
    },
    {
        "id": 9,
        "title": "LLM09:2025 Misinformation",
        "description": "Misinformation highlights the risk of large language models (LLMs) generating inaccurate, misleading, or harmful information. This vulnerability arises from limitations in training data, which may contain outdated or biased content, and from the LLM's tendency to 'hallucinate' information when responding to queries. Misinformation generated by LLMs can lead to trust issues, reputational damage, or dangerous real-world consequences if relied upon in critical decision-making.",
        "task": "This model is trained with a dataset which creates low quality outputs, which may also be misinformation, that can turn out to be harmful. Try asking a few questions to the bot."
    },
    {
        "id": 10,
        "title": "LLM10:2025 Unbounded Consumption",
        "description": "Unbounded Consumption refers to the risk of large language models (LLMs) being used to process unlimited input data or generate excessive outputs without restrictions. This can lead to resource exhaustion, performance degradation, and denial-of-service (DoS) conditions, especially in shared or constrained environments. Attackers may exploit this vulnerability by submitting overly complex or large inputs to overwhelm the system.",
        "task": "Turn on vulnerability mode and ask the bot to tell a never ending story."
    }
]

    return JsonResponse(jsondata, safe=False)